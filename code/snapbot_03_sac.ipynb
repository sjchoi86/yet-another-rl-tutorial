{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8868d91",
   "metadata": {},
   "source": [
    "### Make `Snapbot` walk using `SAC`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a076ee46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MuJoCo version:[2.3.6]\n"
     ]
    }
   ],
   "source": [
    "import mujoco,torch,os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mujoco_parser import MuJoCoParserClass\n",
    "from snapbot_env import SnapbotMarkovDecisionProcessClass\n",
    "from sac import ReplayBufferClass,ActorClass,CriticClass,get_target\n",
    "np.set_printoptions(precision=2,suppress=True,linewidth=100)\n",
    "plt.rc('xtick',labelsize=6); plt.rc('ytick',labelsize=6)\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline\n",
    "print (\"MuJoCo version:[%s]\"%(mujoco.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81aa88e",
   "metadata": {},
   "source": [
    "### Initialize `Snapbot` environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5cb67390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Snapbot] Instantiated\n",
      "   [info] dt:[0.0200] HZ:[50], env-HZ:[500], mujoco_nstep:[10], state_dim:[35], o_dim:[70], a_dim:[8]\n",
      "   [history] total_sec:[0.20]sec, n:[10], intv_sec:[0.10]sec, intv_tick:[5]\n",
      "   [history] ticks:[0 5]\n"
     ]
    }
   ],
   "source": [
    "xml_path = '../asset/snapbot/scene_snapbot.xml'\n",
    "env = MuJoCoParserClass(name='Snapbot',rel_xml_path=xml_path,VERBOSE=False)\n",
    "mdp = SnapbotMarkovDecisionProcessClass(env,HZ=50,history_total_sec=0.2,history_intv_sec=0.1,VERBOSE=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acffff99",
   "metadata": {},
   "source": [
    "### `SAC` hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94fe2df7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_episode:[1000], max_epi_sec:[5.00], max_epi_tick:[250]\n",
      "n_warmup_epi:[10], buffer_limit:[50000], buffer_warmup:[10000]\n"
     ]
    }
   ],
   "source": [
    "n_episode         = 1000 # number of total episodes (rollouts)\n",
    "max_epi_sec       = 5.0 # maximum episode length in second\n",
    "max_epi_tick      = int(max_epi_sec*mdp.HZ) # maximum episode length in tick\n",
    "n_warmup_epi      = 10 # number of warm-up episodes\n",
    "buffer_limit      = 50000\n",
    "buffer_warmup     = buffer_limit // 5\n",
    "init_alpha        = 0.1\n",
    "max_torque        = 2.0\n",
    "# Update\n",
    "lr_actor          = 0.0002\n",
    "lr_alpha          = 0.0000 # 0.0003\n",
    "lr_critic         = 0.0001\n",
    "n_update_per_tick = 1 # number of updates per tick\n",
    "batch_size        = 256\n",
    "gamma             = 0.99\n",
    "tau               = 0.005\n",
    "# Debug\n",
    "print_every       = 20\n",
    "eval_every        = 50\n",
    "RENDER_EVAL       = False\n",
    "save_every        = 50\n",
    "print (\"n_episode:[%d], max_epi_sec:[%.2f], max_epi_tick:[%d]\"%\n",
    "       (n_episode,max_epi_sec,max_epi_tick))\n",
    "print (\"n_warmup_epi:[%d], buffer_limit:[%.d], buffer_warmup:[%d]\"%\n",
    "       (n_warmup_epi,buffer_limit,buffer_warmup))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37845bb4",
   "metadata": {},
   "source": [
    "### Initialize networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04172358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready.\n"
     ]
    }
   ],
   "source": [
    "device = 'cpu' # cpu / mps / cuda\n",
    "replay_buffer = ReplayBufferClass(buffer_limit, device=device)\n",
    "actor = ActorClass(\n",
    "    obs_dim=mdp.o_dim,h_dims=[256,256],out_dim=mdp.a_dim,max_out=max_torque,\n",
    "    init_alpha=init_alpha,lr_actor=lr_actor,lr_alpha=lr_alpha,device=device).to(device)\n",
    "critic_one = CriticClass(\n",
    "    obs_dim=mdp.o_dim,a_dim=mdp.a_dim,h_dims=[256,256],out_dim=1,\n",
    "    lr_critic=lr_critic, device=device).to(device)\n",
    "critic_two = CriticClass(\n",
    "    obs_dim=mdp.o_dim,a_dim=mdp.a_dim,h_dims=[256,256],out_dim=1,\n",
    "    lr_critic=lr_critic, device=device).to(device)\n",
    "critic_one_trgt = CriticClass(\n",
    "    obs_dim=mdp.o_dim,a_dim=mdp.a_dim,h_dims=[256,256],out_dim=1,\n",
    "    lr_critic=lr_critic, device=device).to(device)\n",
    "critic_two_trgt = CriticClass(\n",
    "    obs_dim=mdp.o_dim,a_dim=mdp.a_dim,h_dims=[256,256],out_dim=1,\n",
    "    lr_critic=lr_critic, device=device).to(device)\n",
    "print (\"Ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424625ee",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c63d8ffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready.\n"
     ]
    }
   ],
   "source": [
    "def np2torch(x_np,device): return torch.tensor(x_np,dtype=torch.float32,device=device)\n",
    "def torch2np(x_torch): return x_torch.detach().cpu().numpy()\n",
    "print (\"Ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f99cf6",
   "metadata": {},
   "source": [
    "### Modify torque ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6cb2e130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mdp.env.ctrl_ranges:\n",
      " [[-2.  2.]\n",
      " [-2.  2.]\n",
      " [-2.  2.]\n",
      " [-2.  2.]\n",
      " [-2.  2.]\n",
      " [-2.  2.]\n",
      " [-2.  2.]\n",
      " [-2.  2.]]\n"
     ]
    }
   ],
   "source": [
    "mdp.env.ctrl_ranges[:,0] = -max_torque\n",
    "mdp.env.ctrl_ranges[:,1] = +max_torque\n",
    "print (\"mdp.env.ctrl_ranges:\\n\",mdp.env.ctrl_ranges)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376192ff",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c4afa59",
   "metadata": {},
   "outputs": [],
   "source": [
    "REMOVE_EXISTING_PTH = False\n",
    "SAVE_CURRENT_PTH    = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2141249e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training.\n",
      "[0/1000][0.0%] reward:[-69.155] x_diff:[-0.225] epi_len:[249/250] buffer_size:[250] alpha:[0.10]\n",
      "  [Eval] reward:[3.593] x_diff:[0.004] epi_len:[249/250]\n",
      "  [Save] Remove existing [0] pth files.\n",
      "  [Save] [../result/weights/sac_snapbot/episode_0.pth] saved.\n",
      "[20/1000][2.0%] reward:[-18.607] x_diff:[0.088] epi_len:[249/250] buffer_size:[5211] alpha:[0.10]\n",
      "[40/1000][4.0%] reward:[-4.445] x_diff:[-0.101] epi_len:[249/250] buffer_size:[9625] alpha:[0.10]\n",
      "  [Eval] reward:[9.657] x_diff:[0.096] epi_len:[249/250]\n",
      "  [Save] [../result/weights/sac_snapbot/episode_50.pth] saved.\n",
      "[60/1000][6.0%] reward:[-8.570] x_diff:[-0.049] epi_len:[249/250] buffer_size:[14625] alpha:[0.10]\n",
      "[80/1000][8.0%] reward:[-23.267] x_diff:[0.072] epi_len:[249/250] buffer_size:[19625] alpha:[0.10]\n",
      "[100/1000][10.0%] reward:[7.789] x_diff:[0.433] epi_len:[249/250] buffer_size:[23806] alpha:[0.10]\n",
      "  [Eval] reward:[28.161] x_diff:[0.758] epi_len:[249/250]\n",
      "  [Save] [../result/weights/sac_snapbot/episode_100.pth] saved.\n",
      "[120/1000][12.0%] reward:[-86.819] x_diff:[0.188] epi_len:[249/250] buffer_size:[28806] alpha:[0.10]\n",
      "[140/1000][14.0%] reward:[-9.551] x_diff:[0.018] epi_len:[249/250] buffer_size:[33599] alpha:[0.10]\n",
      "  [Eval] reward:[7.247] x_diff:[0.064] epi_len:[249/250]\n",
      "  [Save] [../result/weights/sac_snapbot/episode_150.pth] saved.\n",
      "[160/1000][16.0%] reward:[5.122] x_diff:[0.061] epi_len:[249/250] buffer_size:[38599] alpha:[0.10]\n",
      "[180/1000][18.0%] reward:[-10.537] x_diff:[0.215] epi_len:[249/250] buffer_size:[43599] alpha:[0.10]\n",
      "[200/1000][20.0%] reward:[2.528] x_diff:[0.102] epi_len:[249/250] buffer_size:[48599] alpha:[0.10]\n",
      "  [Eval] reward:[-68.451] x_diff:[0.140] epi_len:[249/250]\n",
      "  [Save] [../result/weights/sac_snapbot/episode_200.pth] saved.\n",
      "[220/1000][22.0%] reward:[-16.824] x_diff:[0.202] epi_len:[249/250] buffer_size:[50000] alpha:[0.10]\n",
      "[240/1000][24.0%] reward:[21.182] x_diff:[0.416] epi_len:[249/250] buffer_size:[50000] alpha:[0.10]\n",
      "  [Eval] reward:[-24.339] x_diff:[0.370] epi_len:[249/250]\n",
      "  [Save] [../result/weights/sac_snapbot/episode_250.pth] saved.\n",
      "[260/1000][26.0%] reward:[11.290] x_diff:[0.191] epi_len:[249/250] buffer_size:[50000] alpha:[0.10]\n",
      "[280/1000][28.0%] reward:[18.469] x_diff:[0.374] epi_len:[249/250] buffer_size:[50000] alpha:[0.10]\n",
      "[300/1000][30.0%] reward:[26.763] x_diff:[0.657] epi_len:[249/250] buffer_size:[50000] alpha:[0.10]\n",
      "  [Eval] reward:[25.814] x_diff:[0.796] epi_len:[249/250]\n",
      "  [Save] [../result/weights/sac_snapbot/episode_300.pth] saved.\n",
      "[320/1000][32.0%] reward:[4.177] x_diff:[0.083] epi_len:[249/250] buffer_size:[50000] alpha:[0.10]\n",
      "[340/1000][34.0%] reward:[8.818] x_diff:[0.875] epi_len:[249/250] buffer_size:[50000] alpha:[0.10]\n",
      "  [Eval] reward:[-6.468] x_diff:[0.632] epi_len:[249/250]\n",
      "  [Save] [../result/weights/sac_snapbot/episode_350.pth] saved.\n",
      "[360/1000][36.0%] reward:[46.524] x_diff:[1.484] epi_len:[249/250] buffer_size:[50000] alpha:[0.10]\n",
      "[380/1000][38.0%] reward:[10.532] x_diff:[0.606] epi_len:[74/250] buffer_size:[50000] alpha:[0.10]\n",
      "[400/1000][40.0%] reward:[31.034] x_diff:[0.944] epi_len:[249/250] buffer_size:[50000] alpha:[0.10]\n",
      "  [Eval] reward:[64.329] x_diff:[1.707] epi_len:[249/250]\n",
      "  [Save] [../result/weights/sac_snapbot/episode_400.pth] saved.\n",
      "[420/1000][42.0%] reward:[71.791] x_diff:[1.837] epi_len:[249/250] buffer_size:[50000] alpha:[0.10]\n",
      "[440/1000][44.0%] reward:[72.520] x_diff:[2.026] epi_len:[249/250] buffer_size:[50000] alpha:[0.10]\n",
      "  [Eval] reward:[96.846] x_diff:[2.162] epi_len:[249/250]\n",
      "  [Save] [../result/weights/sac_snapbot/episode_450.pth] saved.\n",
      "[460/1000][46.0%] reward:[104.427] x_diff:[2.144] epi_len:[249/250] buffer_size:[50000] alpha:[0.10]\n",
      "[480/1000][48.0%] reward:[-45.629] x_diff:[-0.139] epi_len:[249/250] buffer_size:[50000] alpha:[0.10]\n",
      "[500/1000][50.0%] reward:[80.742] x_diff:[2.059] epi_len:[249/250] buffer_size:[50000] alpha:[0.10]\n",
      "  [Eval] reward:[92.191] x_diff:[1.893] epi_len:[249/250]\n",
      "  [Save] [../result/weights/sac_snapbot/episode_500.pth] saved.\n",
      "[520/1000][52.0%] reward:[108.276] x_diff:[2.196] epi_len:[249/250] buffer_size:[50000] alpha:[0.10]\n",
      "[540/1000][54.0%] reward:[102.477] x_diff:[2.391] epi_len:[249/250] buffer_size:[50000] alpha:[0.10]\n",
      "  [Eval] reward:[87.121] x_diff:[1.987] epi_len:[249/250]\n",
      "  [Save] [../result/weights/sac_snapbot/episode_550.pth] saved.\n",
      "[560/1000][56.0%] reward:[101.176] x_diff:[2.205] epi_len:[249/250] buffer_size:[50000] alpha:[0.10]\n",
      "[580/1000][58.0%] reward:[-16.961] x_diff:[0.662] epi_len:[92/250] buffer_size:[50000] alpha:[0.10]\n",
      "[600/1000][60.0%] reward:[109.554] x_diff:[2.581] epi_len:[249/250] buffer_size:[50000] alpha:[0.10]\n",
      "  [Eval] reward:[139.972] x_diff:[3.010] epi_len:[249/250]\n",
      "  [Save] [../result/weights/sac_snapbot/episode_600.pth] saved.\n",
      "[620/1000][62.0%] reward:[108.574] x_diff:[2.380] epi_len:[249/250] buffer_size:[50000] alpha:[0.10]\n",
      "[640/1000][64.0%] reward:[84.761] x_diff:[2.027] epi_len:[249/250] buffer_size:[50000] alpha:[0.10]\n",
      "  [Eval] reward:[94.645] x_diff:[2.963] epi_len:[249/250]\n",
      "  [Save] [../result/weights/sac_snapbot/episode_650.pth] saved.\n",
      "[660/1000][66.0%] reward:[111.028] x_diff:[2.565] epi_len:[249/250] buffer_size:[50000] alpha:[0.10]\n",
      "[680/1000][68.0%] reward:[123.766] x_diff:[2.876] epi_len:[249/250] buffer_size:[50000] alpha:[0.10]\n",
      "[700/1000][70.0%] reward:[119.402] x_diff:[2.761] epi_len:[249/250] buffer_size:[50000] alpha:[0.10]\n",
      "  [Eval] reward:[118.230] x_diff:[2.392] epi_len:[249/250]\n",
      "  [Save] [../result/weights/sac_snapbot/episode_700.pth] saved.\n",
      "[720/1000][72.0%] reward:[129.982] x_diff:[2.588] epi_len:[249/250] buffer_size:[50000] alpha:[0.10]\n",
      "[740/1000][74.0%] reward:[154.482] x_diff:[3.118] epi_len:[249/250] buffer_size:[50000] alpha:[0.10]\n",
      "  [Eval] reward:[95.710] x_diff:[2.458] epi_len:[249/250]\n",
      "  [Save] [../result/weights/sac_snapbot/episode_750.pth] saved.\n",
      "[760/1000][76.0%] reward:[128.137] x_diff:[2.749] epi_len:[249/250] buffer_size:[50000] alpha:[0.10]\n",
      "[780/1000][78.0%] reward:[149.893] x_diff:[3.073] epi_len:[249/250] buffer_size:[50000] alpha:[0.10]\n",
      "[800/1000][80.0%] reward:[120.550] x_diff:[2.389] epi_len:[249/250] buffer_size:[50000] alpha:[0.10]\n",
      "  [Eval] reward:[159.140] x_diff:[3.186] epi_len:[249/250]\n",
      "  [Save] [../result/weights/sac_snapbot/episode_800.pth] saved.\n",
      "[820/1000][82.0%] reward:[153.971] x_diff:[3.065] epi_len:[249/250] buffer_size:[50000] alpha:[0.10]\n",
      "[840/1000][84.0%] reward:[101.656] x_diff:[2.204] epi_len:[249/250] buffer_size:[50000] alpha:[0.10]\n",
      "  [Eval] reward:[138.812] x_diff:[2.928] epi_len:[249/250]\n",
      "  [Save] [../result/weights/sac_snapbot/episode_850.pth] saved.\n",
      "[860/1000][86.0%] reward:[166.067] x_diff:[3.329] epi_len:[249/250] buffer_size:[50000] alpha:[0.10]\n",
      "[880/1000][88.0%] reward:[186.692] x_diff:[3.742] epi_len:[249/250] buffer_size:[50000] alpha:[0.10]\n",
      "[900/1000][90.0%] reward:[127.582] x_diff:[2.604] epi_len:[249/250] buffer_size:[50000] alpha:[0.10]\n",
      "  [Eval] reward:[-1682.960] x_diff:[0.618] epi_len:[249/250]\n",
      "  [Save] [../result/weights/sac_snapbot/episode_900.pth] saved.\n",
      "[920/1000][92.0%] reward:[-6.714] x_diff:[0.732] epi_len:[249/250] buffer_size:[50000] alpha:[0.10]\n",
      "[940/1000][94.0%] reward:[167.256] x_diff:[3.605] epi_len:[249/250] buffer_size:[50000] alpha:[0.10]\n",
      "  [Eval] reward:[97.904] x_diff:[2.142] epi_len:[249/250]\n",
      "  [Save] [../result/weights/sac_snapbot/episode_950.pth] saved.\n",
      "[960/1000][96.0%] reward:[135.259] x_diff:[2.917] epi_len:[249/250] buffer_size:[50000] alpha:[0.10]\n",
      "[980/1000][98.0%] reward:[161.687] x_diff:[3.229] epi_len:[249/250] buffer_size:[50000] alpha:[0.10]\n",
      "[999/1000][99.9%] reward:[153.099] x_diff:[3.274] epi_len:[249/250] buffer_size:[50000] alpha:[0.10]\n",
      "  [Eval] reward:[186.530] x_diff:[4.129] epi_len:[249/250]\n",
      "  [Save] [../result/weights/sac_snapbot/episode_999.pth] saved.\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "print (\"Start training.\")\n",
    "for epi_idx in range(n_episode): # for each episode\n",
    "    zere_to_one = epi_idx/n_episode\n",
    "    one_to_zero = 1-zere_to_one\n",
    "    # Reset MDP\n",
    "    s = mdp.reset()\n",
    "    # Loop\n",
    "    USE_RANDOM_POLICY = (np.random.rand()<(0.1*one_to_zero)) or (epi_idx < n_warmup_epi)\n",
    "    reward_total,reward_forward = 0.0,0.0\n",
    "    for tick in range(max_epi_tick): # for each tick in an episode\n",
    "        if USE_RANDOM_POLICY:\n",
    "            a_np = mdp.sample_action()\n",
    "        else:\n",
    "            a,log_prob = actor(np2torch(s,device=device))\n",
    "            a_np = torch2np(a)\n",
    "        # Step\n",
    "        s_prime,reward,done,info = mdp.step(a_np,max_time=max_epi_sec)\n",
    "        replay_buffer.put((s,a_np,reward,s_prime,done))\n",
    "        reward_total += reward \n",
    "        reward_forward += info['r_forward']\n",
    "        s = s_prime\n",
    "        if done is True: break # terminate condition\n",
    "        \n",
    "        # Replay buffer\n",
    "        if replay_buffer.size() > buffer_warmup:\n",
    "             for _ in range(n_update_per_tick): \n",
    "                mini_batch = replay_buffer.sample(batch_size)\n",
    "                # Update critics\n",
    "                td_target = get_target(actor,critic_one_trgt,critic_two_trgt,\n",
    "                                       gamma=gamma,mini_batch=mini_batch,device=device)\n",
    "                critic_one.train(td_target,mini_batch)\n",
    "                critic_two.train(td_target,mini_batch)\n",
    "                # Update actor\n",
    "                actor.train(critic_one,critic_two,target_entropy=-mdp.a_dim,mini_batch=mini_batch)\n",
    "                # Soft update of critics\n",
    "                critic_one.soft_update(tau=tau,net_target=critic_one_trgt)\n",
    "                critic_two.soft_update(tau=tau,net_target=critic_two_trgt)\n",
    "\n",
    "    # Compute x_diff\n",
    "    x_diff = mdp.env.get_p_body('torso')[0]\n",
    "        \n",
    "    # Print\n",
    "    if (epi_idx%print_every)==0 or (epi_idx==(n_episode-1)):\n",
    "        epi_tick = tick\n",
    "        print (\"[%d/%d][%.1f%%] reward:[%.3f] x_diff:[%.3f] epi_len:[%d/%d] buffer_size:[%d] alpha:[%.2f]\"%\n",
    "               (epi_idx,n_episode,100.0*(epi_idx/n_episode),reward_total,x_diff,epi_tick,max_epi_tick,\n",
    "                replay_buffer.size(),actor.log_alpha.exp()))\n",
    "    \n",
    "    # Evaluation\n",
    "    if (epi_idx%eval_every)==0 or (epi_idx==(n_episode-1)):\n",
    "        if RENDER_EVAL: mdp.init_viewer()\n",
    "        s = mdp.reset()\n",
    "        reward_total = 0.0\n",
    "        for tick in range(max_epi_tick):\n",
    "            a,_ = actor(np2torch(s,device=device),SAMPLE_ACTION=False)\n",
    "            s_prime,reward,done,info = mdp.step(torch2np(a),max_time=max_epi_sec)\n",
    "            reward_total += reward\n",
    "            if RENDER_EVAL and ((tick%5) == 0):\n",
    "                mdp.render(TRACK_TORSO=True,PLOT_WORLD_COORD=True,PLOT_TORSO_COORD=True,\n",
    "                           PLOT_SENSOR=True,PLOT_CONTACT=True,PLOT_TIME=True)\n",
    "            s = s_prime\n",
    "            if RENDER_EVAL: if not mdp.is_viewer_alive(): break\n",
    "        if RENDER_EVAL: mdp.close_viewer()\n",
    "        x_diff = mdp.env.get_p_body('torso')[0]\n",
    "        print (\"  [Eval] reward:[%.3f] x_diff:[%.3f] epi_len:[%d/%d]\"%\n",
    "               (reward_total,x_diff,tick,max_epi_tick))\n",
    "\n",
    "    # Save network\n",
    "    if (epi_idx%save_every)==0 or (epi_idx==(n_episode-1)):\n",
    "        pth_path = '../result/weights/sac_%s/episode_%d.pth'%(mdp.name.lower(),epi_idx)\n",
    "        dir_path = os.path.dirname(pth_path)\n",
    "        if not os.path.exists(dir_path): os.makedirs(dir_path)\n",
    "        if epi_idx == 0: # remove all existing files (if epi_idx is 0)\n",
    "            files = os.listdir(path=dir_path)\n",
    "            if REMOVE_EXISTING_PTH:\n",
    "                print (\"  [Save] Remove existing [%d] pth files.\"%(len(files)))\n",
    "                for file in files: os.remove(os.path.join(dir_path,file))\n",
    "        if SAVE_CURRENT_PTH:\n",
    "            torch.save(actor.state_dict(),pth_path)\n",
    "            print (\"  [Save] [%s] saved.\"%(pth_path))\n",
    "\n",
    "print (\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590b7ba1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
